{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 4 : Tree Based Models\n",
    "## Manuel Serna-Aguilera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do following 3 Tasks:\n",
    "1. An example of Decsion Tree\n",
    "2. Bagging, Random Forest and Boosting for Regression\n",
    "3. Random Forest and Boosting for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "1. For the Hitters Data we discussed in the class, split the data into 80-20 %  training and validation data. (You can get the data from the Github repo I provided in the beginning of the class.)\n",
    "2. Fit a Decision Tree with Number of leaf nodes 5, 10, 15, 20, .... 50. For each model compute training error and validation errror (MSE). Plot training a graph with number of leaves vs Training and Validation MSE. \n",
    "\n",
    "Food for thought    \n",
    "1. When we discussed Decision Treee in the class, the book mentions building decision tree are not computationally expensive. Imagine you have 1000 varaibles and 10,000 data samples(training data), sounds like a very computationally expensive task. Why the book might have the argument that decision trees are not computationally expensive?\n",
    "2. Do you see any trend on training MSE and validation MSE as numbers of leaves increases? What happens if you increase the number of leaves to number of training data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) i) Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_df = pd.read_csv('Hitters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>185</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>214</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>76</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>70.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>298</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>509</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>121</td>\n",
       "      <td>283</td>\n",
       "      <td>9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>323</td>\n",
       "      <td>81</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>341</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>143</td>\n",
       "      <td>290</td>\n",
       "      <td>19</td>\n",
       "      <td>75.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>401</td>\n",
       "      <td>92</td>\n",
       "      <td>17</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>13</td>\n",
       "      <td>5206</td>\n",
       "      <td>1332</td>\n",
       "      <td>253</td>\n",
       "      <td>784</td>\n",
       "      <td>890</td>\n",
       "      <td>866</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "0    293    66      1    30   29     14      1     293     66       1     30   \n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "6    185    37      1    23    8     21      2     214     42       1     30   \n",
       "7    298    73      0    24   24      7      3     509    108       0     41   \n",
       "8    323    81      6    26   32      8      2     341     86       6     32   \n",
       "9    401    92     17    49   66     65     13    5206   1332     253    784   \n",
       "\n",
       "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
       "0    29      14      A        E      446       33      20     NaN         A  \n",
       "1   414     375      N        W      632       43      10   475.0         N  \n",
       "2   266     263      A        W      880       82      14   480.0         A  \n",
       "3   838     354      N        E      200       11       3   500.0         N  \n",
       "4    46      33      N        E      805       40       4    91.5         N  \n",
       "5   336     194      A        W      282      421      25   750.0         A  \n",
       "6     9      24      N        E       76      127       7    70.0         A  \n",
       "7    37      12      A        W      121      283       9   100.0         A  \n",
       "8    34       8      N        W      143      290      19    75.0         N  \n",
       "9   890     866      A        E        0        0       0  1100.0         A  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display several entries\n",
    "hit_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the three categorical variables with dummy variables\n",
    "hit_df = pd.get_dummies(hit_df, prefix=['League', 'Division', 'NewLeague'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null entries\n",
    "hit_df = hit_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'Salary'\n",
    "y = hit_df[[output_name]].values # get salary--our output to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = hit_df.drop(output_name, axis=1).values # all other columns are input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale input and output\n",
    "x = StandardScaler().fit_transform(x)\n",
    "y = StandardScaler().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) ii) Fit Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y):\n",
    "    pred = model.predict(x)\n",
    "    diff = y - pred\n",
    "    return (diff**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trees with number of leaves increasing by 5\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "\n",
    "reg_trees = [DTR(max_leaf_nodes = i, random_state=42) for i in range(5, 55, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit trees to data\n",
    "N = len(reg_trees)\n",
    "\n",
    "for i in range(N):\n",
    "    reg_trees[i].fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSE\n",
    "train_errors = [0.0 for i in range(N)]\n",
    "val_errors = [0.0 for i in range(N)]\n",
    "\n",
    "for i in range(N):\n",
    "    train_errors[i] = evaluate(reg_trees[i], x_train, y_train)\n",
    "    val_errors[i] = evaluate(reg_trees[i], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(5, 55, 5), train_errors, label='train error')\n",
    "plt.plot(range(5, 55, 5), val_errors, label='val error')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Number of Leaves in Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.title('Leaves vs MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Boston Housing Data Set\n",
    "(You can downlaod dataset from http://lib.stat.cmu.edu/datasets/boston or import from sklearn.dataset)\n",
    "1. For a Boston Housing Data Set, split data into 80-20 % train and test. Perform Random Forest on Training Data. Compute Train Error, Out of Sample Error and Test Error (ALL MSE). \n",
    "When you do Random Forest, you will see the model depends on following three parameters: \n",
    "a. n_estimators : Numbers of Trees\n",
    "b. min_samples_leaf : Minimum  numbers of samples in the leaf nodes. \n",
    "c. max_features : Numbers of Features to consider at each split during tree building.    \n",
    "You can use RandomizedSearchCV from sklearn to perform grid search and find the best model pararamters.    \n",
    "\n",
    "(Here is a good resource on Random Forest, Feature selection and parameter optimization that uses Boston Housing Dataset as an example.)      \n",
    "https://towardsdatascience.com/predicting-housing-prices-using-a-scikit-learns-random-forest-model-e736b59d56c5     \n",
    "\n",
    "2. Perform Boosting (You can use XG Boost) and compare your best validation MSE from Random Forest to Validation MSE from Boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) i) a) Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest regressor with pre-set parameters\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "num_trees = 100 # number of estimators to use in random forest\n",
    "\n",
    "# NOTE: total features to consider is sqrt of total features\n",
    "# NOTE: using at least half of features to make splits\n",
    "forest = RFR(\n",
    "    n_estimators=num_trees, \n",
    "    min_samples_leaf=1, \n",
    "    max_features='sqrt',\n",
    "    oob_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features='sqrt', oob_score=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) i) b) Get traininig error, out-of-sample (out-of-bag) error, and testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.76467130693069"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute training error\n",
    "f_train_error = evaluate(forest, x_train, y_train)\n",
    "f_train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14417332041029907"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute out-of-sample error\n",
    "f_oob_error = 1 - forest.oob_score_\n",
    "f_oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.404608862745097"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute test error\n",
    "f_test_error = evaluate(forest, x_test, y_test)\n",
    "f_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) ii) a) Perform boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBRegressor(\n",
    "    #n_estimators=num_trees, \n",
    "    #max_depth=5,\n",
    "    #eta=1,\n",
    "    verbosity=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=6,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.237269410289812"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get validation (same as test set) MSE\n",
    "boost_pred = clf.predict(x_test)\n",
    "boost_mse = evaluate(clf, x_test, y_test)\n",
    "boost_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) ii) b) Compare best validation MSE from Random Forest to validation MSE from Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val MSE (Random Forest):5.404609\n",
      "Val MSE (Boosting):\t6.237269\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Val MSE (Random Forest):{:.6f}\\nVal MSE (Boosting):\\t{:.6f}'.format(f_test_error, boost_mse)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2) ii) b) Response\n",
    "Based on the above comparison, it seems the validation MSE for random forest and boosting is approximately the same. At the current scale, Boosting is somewhat better (but not by a lot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Classification of Wine Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "                      'machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                   'Proline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0            1    14.23        1.71  2.43               15.6        127   \n",
       "1            1    13.20        1.78  2.14               11.2        100   \n",
       "2            1    13.16        2.36  2.67               18.6        101   \n",
       "3            1    14.37        1.95  2.50               16.8        113   \n",
       "4            1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Drop one of the classes so we have only two classes. \n",
    "df_wine = df_wine[df_wine['Class label'] != 1]\n",
    "\n",
    "y = df_wine['Class label'].values\n",
    "X = df_wine[['Alcohol', 'OD280/OD315 of diluted wines']].values\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Labels for classes, Encode from 2/3 to 0/1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "#print(y)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3, Do your part here\n",
    "Perform Classification using Decision Tree and Bagging Classifier. Use following params      \n",
    "\n",
    "For Decision Tree    \n",
    "\n",
    "criterion='entropy'    \n",
    "max_depth=None    \n",
    "random_state=1    \n",
    "\n",
    "\n",
    "For Bagging Classifier     \n",
    "base_estimator=tree   \n",
    "n_estimators=500    \n",
    "max_samples=1.0  \n",
    "max_features=1.0    \n",
    "bootstrap=True   \n",
    "bootstrap_features=False      \n",
    "n_jobs=1       \n",
    "random_state=1\n",
    "\n",
    "Print Out your train and test accuracies for Decision Tree and Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=None,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit bagging classifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=1.0,\n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    n_jobs=1,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=1, random_state=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train and test accuracies for both classifiers\n",
    "tree_train_acc = tree_clf.score(X_train, y_train)\n",
    "tree_test_acc = tree_clf.score(X_test, y_test)\n",
    "\n",
    "bag_train_acc = bag_clf.score(X_train, y_train)\n",
    "bag_test_acc = bag_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Train Acc: 1.000000\n",
      "Test Acc:  0.833333\n",
      "Bagging:\n",
      "Train Acc: 1.000000\n",
      "Test Acc:  0.916667\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree:')\n",
    "print('Train Acc: {:.6f}'.format(tree_train_acc))\n",
    "print('Test Acc:  {:.6f}'.format(tree_test_acc))\n",
    "\n",
    "print('Bagging:')\n",
    "print('Train Acc: {:.6f}'.format(bag_train_acc))\n",
    "print('Test Acc:  {:.6f}'.format(bag_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, it clear that for previously unseen data samples, bagging is able to classify significantly better than a single decision tree classifier. This makes sense as we are bootstrapping with 500 different tree classifiers in the bag classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
